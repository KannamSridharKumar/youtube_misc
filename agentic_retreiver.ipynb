{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b7abab",
   "metadata": {},
   "source": [
    "### Agentic Retriever\n",
    "- Reduce the cost\n",
    "- improve the speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Enterprise RAG\n",
    "\n",
    "Souce:  - # of chunks\n",
    "Product  - 10K\n",
    "User Guides - 10K\n",
    "Developer Guides - 10K\n",
    "QA - 10K\n",
    "Chat History - 10K\n",
    "\n",
    "Total - 50K Chunks\n",
    "\n",
    "Query: \n",
    "- How do I create a new feature?\n",
    "- Which product is best suited for xyz usecase?\n",
    "- How do I use this feature?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d116a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af0fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "source_docs = [\n",
    "    Document(\n",
    "        page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]}\n",
    "    ) for doc in knowledge_base\n",
    "]\n",
    "\n",
    "docs_processed = RecursiveCharacterTextSplitter(chunk_size=500).split_documents(source_docs)[:1000]\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=docs_processed,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d7f46dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:', metadata={'source': 'hf-endpoints-documentation'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f99f9d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'hf-endpoints-documentation'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_processed[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b895c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "n_chunks = len(docs_processed)\n",
    "print(n_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d1eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['peft', 'gradio', 'diffusers', 'datasets', 'deep-rl-class', 'blog', 'course', 'optimum', 'hub-docs', 'hf-endpoints-documentation', 'evaluate', 'transformers', 'datasets-server', 'pytorch-image-models']\n"
     ]
    }
   ],
   "source": [
    "all_sources = list(set([doc.metadata[\"source\"] for doc in docs_processed]))\n",
    "print(all_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a12f8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'transformers': 439, 'deep-rl-class': 74, 'evaluate': 72, 'pytorch-image-models': 69, 'datasets-server': 65, 'gradio': 64, 'blog': 58, 'diffusers': 57, 'course': 38, 'datasets': 32, 'hub-docs': 18, 'hf-endpoints-documentation': 9, 'peft': 3, 'optimum': 2})\n"
     ]
    }
   ],
   "source": [
    "all_sources_2 = list([doc.metadata[\"source\"] for doc in docs_processed])\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter(all_sources_2)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "225b06a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'transformers': 439,\n",
       "         'deep-rl-class': 74,\n",
       "         'evaluate': 72,\n",
       "         'pytorch-image-models': 69,\n",
       "         'datasets-server': 65,\n",
       "         'gradio': 64,\n",
       "         'blog': 58,\n",
       "         'diffusers': 57,\n",
       "         'course': 38,\n",
       "         'datasets': 32,\n",
       "         'hub-docs': 18,\n",
       "         'hf-endpoints-documentation': 9,\n",
       "         'peft': 3,\n",
       "         'optimum': 2})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5188975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers.agents import Tool\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"text\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        },\n",
    "        \"source\": {\n",
    "            \"type\": \"text\", \n",
    "            \"description\": \"\"\n",
    "        },\n",
    "    }\n",
    "    output_type = \"text\"\n",
    "    \n",
    "    def __init__(self, vectordb: VectorStore, all_sources: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vectordb = vectordb\n",
    "        self.inputs[\"source\"][\"description\"] = (\n",
    "            f\"The source of the documents to search, as a str representation of a list. Possible values in the list are: {all_sources}. If this argument is not provided, all sources will be searched.\"\n",
    "          )\n",
    "\n",
    "    def forward(self, query: str, source: str = None) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        if source:\n",
    "            if isinstance(source, str) and \"[\" not in str(source): # if the source is not representing a list\n",
    "                source = [source]\n",
    "            source = json.loads(str(source).replace(\"'\", '\"'))\n",
    "\n",
    "        docs = self.vectordb.similarity_search(query, filter=({\"source\": source} if source else None), k=3)\n",
    "\n",
    "        if len(docs) == 0:\n",
    "            return \"No documents found with this filtering. Try removing the source filter.\"\n",
    "        return \"Retrieved documents:\\n\\n\" + \"\\n===Document===\\n\".join(\n",
    "            [doc.page_content for doc in docs]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3342ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.agents import HfEngine, ReactJsonAgent\n",
    "\n",
    "llm_engine = HfEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "\n",
    "agent = ReactJsonAgent(\n",
    "    tools=[RetrieverTool(vectordb, all_sources)],\n",
    "    llm_engine=llm_engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "066fdf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "from transformers.agents.llm_engine import MessageRole, get_clean_message_list\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "openai_role_conversions = {\n",
    "    MessageRole.TOOL_RESPONSE: MessageRole.USER,\n",
    "}\n",
    "\n",
    "\n",
    "class OpenAIEngine:\n",
    "    def __init__(self, model_name=\"gpt-4o-mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        )\n",
    "\n",
    "    def __call__(self, messages, stop_sequences=[]):\n",
    "        messages = get_clean_message_list(\n",
    "            messages, role_conversions=openai_role_conversions\n",
    "        )\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            stop=stop_sequences,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f020cf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mPlease show me a LORA finetuning script\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33;1mCalling tool: 'retriever' with arguments: {'query': 'LORA finetuning script', 'source': 'blog'}\u001b[0m\n",
      "\u001b[33;1mCalling tool: 'retriever' with arguments: {'query': 'LORA finetuning script', 'source': ''}\u001b[0m\n",
      "\u001b[33;1mCalling tool: 'final_answer' with arguments: You can find a LORA finetuning script at the following link: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:\n",
      "You can find a LORA finetuning script at the following link: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py\n"
     ]
    }
   ],
   "source": [
    "#from transformers.agents import HfEngine, ReactJsonAgent\n",
    "\n",
    "#llm_engine = HfEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "#llm_engine = HfEngine(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "#llm_engine = HfEngine(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "llm_engine = OpenAIEngine(model_name=\"gpt-4o-mini\")  \n",
    "\n",
    "\n",
    "agent = ReactJsonAgent(\n",
    "    tools=[RetrieverTool(vectordb, all_sources)],\n",
    "    llm_engine=llm_engine\n",
    ")\n",
    "\n",
    "agent_output = agent.run(\"Please show me a LORA finetuning script\")\n",
    "\n",
    "print(\"Final output:\")\n",
    "print(agent_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "730819a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(agent.logs) #[2]['llm_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0768ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bfc9a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thought: I will search for a document that contains a LORA finetuning script to provide the necessary information.\\nAction:\\n{\\n  \"action\": \"retriever\",\\n  \"action_input\": {\"query\": \"LORA finetuning script\", \"source\": \"blog\"}\\n}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#agent.logs[2] #.keys() #['llm_output']\n",
    "\n",
    "agent.logs[1]['llm_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c36049a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thought: Since there were no documents found with the specific source filter, I will search for a LORA finetuning script without any source restrictions.\\nAction:\\n{\\n  \"action\": \"retriever\",\\n  \"action_input\": {\"query\": \"LORA finetuning script\", \"source\": \"\"}\\n}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.logs[2]['llm_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9818d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "['peft', 'gradio', 'diffusers', 'datasets', 'deep-rl-class', 'blog', 'course', \n",
    " 'optimum', 'hub-docs', 'hf-endpoints-documentation', 'evaluate', 'transformers', \n",
    " 'datasets-server', 'pytorch-image-models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84715540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_prompt': 'You are an expert assistant who can solve any task using JSON tool calls. You will be given a task to solve as best you can.\\nTo do so, you have been given access to the following tools: \\'retriever\\', \\'final_answer\\'\\nThe way you use the tools is by specifying a json blob, ending with \\'<end_action>\\'.\\nSpecifically, this json should have an `action` key (name of the tool to use) and an `action_input` key (input to the tool).\\n\\nThe $ACTION_JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. It should be formatted in json. Do not try to escape special characters. Here is the template of a valid $ACTION_JSON_BLOB:\\n{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}<end_action>\\n\\nMake sure to have the $INPUT as a dictionary in the right format for the tool you are using, and do not put variable names as input if you can find the right values.\\n\\nYou should ALWAYS use the following format:\\n\\nThought: you should always think about one action to take. Then use the action as follows:\\nAction:\\n$ACTION_JSON_BLOB\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $ACTION_JSON_BLOB must only use a SINGLE action at a time.)\\n\\nYou can use the result of the previous action as input for the next action.\\nThe observation will always be a string: it can represent a file, like \"image_1.jpg\".\\nThen you can use it as input for the next action. You can do it for instance as follows:\\n\\nObservation: \"image_1.jpg\"\\n\\nThought: I need to transform the image that I received in the previous observation to make it green.\\nAction:\\n{\\n  \"action\": \"image_transformer\",\\n  \"action_input\": {\"image\": \"image_1.jpg\"}\\n}<end_action>\\n\\nTo provide the final answer to the task, use an action blob with \"action\": \"final_answer\" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:\\nAction:\\n{\\n  \"action\": \"final_answer\",\\n  \"action_input\": {\"answer\": \"insert your final answer here\"}\\n}<end_action>\\n\\n\\nHere are a few examples using notional tools:\\n---\\nTask: \"Generate an image of the oldest person in this document.\"\\n\\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\\nAction:\\n{\\n  \"action\": \"document_qa\",\\n  \"action_input\": {\"document\": \"document.pdf\", \"question\": \"Who is the oldest person mentioned?\"}\\n}<end_action>\\nObservation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\\n\\n\\nThought: I will now generate an image showcasing the oldest person.\\nAction:\\n{\\n  \"action\": \"image_generator\",\\n  \"action_input\": {\"text\": \"\"A portrait of John Doe, a 55-year-old man living in Canada.\"\"}\\n}<end_action>\\nObservation: \"image.png\"\\n\\nThought: I will now return the generated image.\\nAction:\\n{\\n  \"action\": \"final_answer\",\\n  \"action_input\": \"image.png\"\\n}<end_action>\\n\\n---\\nTask: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\\n\\nThought: I will use python code evaluator to compute the result of the operation and then return the final answer using the `final_answer` tool\\nAction:\\n{\\n    \"action\": \"python_interpreter\",\\n    \"action_input\": {\"code\": \"5 + 3 + 1294.678\"}\\n}<end_action>\\nObservation: 1302.678\\n\\nThought: Now that I know the result, I will now return it.\\nAction:\\n{\\n  \"action\": \"final_answer\",\\n  \"action_input\": \"1302.678\"\\n}<end_action>\\n\\n---\\nTask: \"Which city has the highest population , Guangzhou or Shanghai?\"\\n\\nThought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\\nAction:\\n{\\n    \"action\": \"search\",\\n    \"action_input\": \"Population Guangzhou\"\\n}<end_action>\\nObservation: [\\'Guangzhou has a population of 15 million inhabitants as of 2021.\\']\\n\\n\\nThought: Now let\\'s get the population of Shanghai using the tool \\'search\\'.\\nAction:\\n{\\n    \"action\": \"search\",\\n    \"action_input\": \"Population Shanghai\"\\n}\\nObservation: \\'26 million (2019)\\'\\n\\nThought: Now I know that Shanghai has a larger population. Let\\'s return the result.\\nAction:\\n{\\n  \"action\": \"final_answer\",\\n  \"action_input\": \"Shanghai\"\\n}<end_action>\\n\\n\\nAbove example were using notional tools that might not exist for you. You only have acces to those tools:\\n\\n- retriever: Retrieves some documents from the knowledge base that have the closest embeddings to the input query.\\n    Takes inputs: {\\'query\\': {\\'type\\': \\'text\\', \\'description\\': \\'The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\\'}, \\'source\\': {\\'type\\': \\'text\\', \\'description\\': \"The source of the documents to search, as a str representation of a list. Possible values in the list are: [\\'peft\\', \\'gradio\\', \\'diffusers\\', \\'datasets\\', \\'deep-rl-class\\', \\'blog\\', \\'course\\', \\'optimum\\', \\'hub-docs\\', \\'hf-endpoints-documentation\\', \\'evaluate\\', \\'transformers\\', \\'datasets-server\\', \\'pytorch-image-models\\']. If this argument is not provided, all sources will be searched.\"}}\\n\\n- final_answer: Provides a final answer to the given problem.\\n    Takes inputs: {\\'answer\\': {\\'type\\': \\'text\\', \\'description\\': \\'The final answer to the problem\\'}}\\n\\nHere are the rules you should always follow to solve your task:\\n1. ALWAYS provide a \\'Thought:\\' sequence, and an \\'Action:\\' sequence that ends with <end_action>, else you will fail.\\n2. Always use the right arguments for the tools. Never use variable names in the \\'action_input\\' field, use the value instead.\\n3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.\\n4. Never re-do a tool call that you previously did with the exact same parameters.\\n\\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\\n',\n",
       " 'task': 'Please show me a LORA finetuning script'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.logs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d140a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_memory': [{'role': <MessageRole.SYSTEM: 'system'>,\n",
       "   'content': 'You are an expert assistant who can solve any task using JSON tool calls. You will be given a task to solve as best you can.\\nTo do so, you have been given access to the following tools: \\'retriever\\', \\'final_answer\\'\\nThe way you use the tools is by specifying a json blob, ending with \\'<end_action>\\'.\\nSpecifically, this json should have an `action` key (name of the tool to use) and an `action_input` key (input to the tool).\\n\\nThe $ACTION_JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. It should be formatted in json. Do not try to escape special characters. Here is the template of a valid $ACTION_JSON_BLOB:\\n{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}<end_action>\\n\\nMake sure to have the $INPUT as a dictionary in the right format for the tool you are using, and do not put variable names as input if you can find the right values.\\n\\nYou should ALWAYS use the following format:\\n\\nThought: you should always think about one action to take. Then use the action as follows:\\nAction:\\n$ACTION_JSON_BLOB\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $ACTION_JSON_BLOB must only use a SINGLE action at a time.)\\n\\nYou can use the result of the previous action as input for the next action.\\nThe observation will always be a string: it can represent a file, like \"image_1.jpg\".\\nThen you can use it as input for the next action. You can do it for instance as follows:\\n\\nObservation: \"image_1.jpg\"\\n\\nThought: I need to transform the image that I received in the previous observation to make it green.\\nAction:\\n{\\n  \"action\": \"image_transformer\",\\n  \"action_input\": {\"image\": \"image_1.jpg\"}\\n}<end_action>\\n\\nTo provide the final answer to the task, use an action blob with \"action\": \"final_answer\" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:\\nAction:\\n{\\n  \"action\": \"final_answer\",\\n  \"action_input\": {\"answer\": \"insert your final answer here\"}\\n}<end_action>\\n\\n\\nHere are a few examples using notional tools:\\n---\\nTask: \"Generate an image of the oldest person in this document.\"\\n\\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\\nAction:\\n{\\n  \"action\": \"document_qa\",\\n  \"action_input\": {\"document\": \"document.pdf\", \"question\": \"Who is the oldest person mentioned?\"}\\n}<end_action>\\nObservation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\\n\\n\\nThought: I will now generate an image showcasing the oldest person.\\nAction:\\n{\\n  \"action\": \"image_generator\",\\n  \"action_input\": {\"text\": \"\"A portrait of John Doe, a 55-year-old man living in Canada.\"\"}\\n}<end_action>\\nObservation: \"image.png\"\\n\\nThought: I will now return the generated image.\\nAction:\\n{\\n  \"action\": \"final_answer\",\\n  \"action_input\": \"image.png\"\\n}<end_action>\\n\\n---\\nTask: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\\n\\nThought: I will use python code evaluator to compute the result of the operation and then return the final answer using the `final_answer` tool\\nAction:\\n{\\n    \"action\": \"python_interpreter\",\\n    \"action_input\": {\"code\": \"5 + 3 + 1294.678\"}\\n}<end_action>\\nObservation: 1302.678\\n\\nThought: Now that I know the result, I will now return it.\\nAction:\\n{\\n  \"action\": \"final_answer\",\\n  \"action_input\": \"1302.678\"\\n}<end_action>\\n\\n---\\nTask: \"Which city has the highest population , Guangzhou or Shanghai?\"\\n\\nThought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\\nAction:\\n{\\n    \"action\": \"search\",\\n    \"action_input\": \"Population Guangzhou\"\\n}<end_action>\\nObservation: [\\'Guangzhou has a population of 15 million inhabitants as of 2021.\\']\\n\\n\\nThought: Now let\\'s get the population of Shanghai using the tool \\'search\\'.\\nAction:\\n{\\n    \"action\": \"search\",\\n    \"action_input\": \"Population Shanghai\"\\n}\\nObservation: \\'26 million (2019)\\'\\n\\nThought: Now I know that Shanghai has a larger population. Let\\'s return the result.\\nAction:\\n{\\n  \"action\": \"final_answer\",\\n  \"action_input\": \"Shanghai\"\\n}<end_action>\\n\\n\\nAbove example were using notional tools that might not exist for you. You only have acces to those tools:\\n\\n- retriever: Retrieves some documents from the knowledge base that have the closest embeddings to the input query.\\n    Takes inputs: {\\'query\\': {\\'type\\': \\'text\\', \\'description\\': \\'The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\\'}, \\'source\\': {\\'type\\': \\'text\\', \\'description\\': \"The source of the documents to search, as a str representation of a list. Possible values in the list are: [\\'peft\\', \\'gradio\\', \\'diffusers\\', \\'datasets\\', \\'deep-rl-class\\', \\'blog\\', \\'course\\', \\'optimum\\', \\'hub-docs\\', \\'hf-endpoints-documentation\\', \\'evaluate\\', \\'transformers\\', \\'datasets-server\\', \\'pytorch-image-models\\']. If this argument is not provided, all sources will be searched.\"}}\\n\\n- final_answer: Provides a final answer to the given problem.\\n    Takes inputs: {\\'answer\\': {\\'type\\': \\'text\\', \\'description\\': \\'The final answer to the problem\\'}}\\n\\nHere are the rules you should always follow to solve your task:\\n1. ALWAYS provide a \\'Thought:\\' sequence, and an \\'Action:\\' sequence that ends with <end_action>, else you will fail.\\n2. Always use the right arguments for the tools. Never use variable names in the \\'action_input\\' field, use the value instead.\\n3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.\\n4. Never re-do a tool call that you previously did with the exact same parameters.\\n\\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\\n'},\n",
       "  {'role': <MessageRole.USER: 'user'>,\n",
       "   'content': 'Task: Please show me a LORA finetuning script'}],\n",
       " 'llm_output': 'Thought: I will search for a document that contains a LORA finetuning script to provide the necessary information.\\nAction:\\n{\\n  \"action\": \"retriever\",\\n  \"action_input\": {\"query\": \"LORA finetuning script\", \"source\": \"blog\"}\\n}',\n",
       " 'rationale': 'Thought: I will search for a document that contains a LORA finetuning script to provide the necessary information.\\n',\n",
       " 'tool_call': {'tool_name': 'retriever',\n",
       "  'tool_arguments': {'query': 'LORA finetuning script', 'source': 'blog'}},\n",
       " 'observation': 'No documents found with this filtering. Try removing the source filter.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.logs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a0bbed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'transformers': 439,\n",
       "         'deep-rl-class': 74,\n",
       "         'evaluate': 72,\n",
       "         'pytorch-image-models': 69,\n",
       "         'datasets-server': 65,\n",
       "         'gradio': 64,\n",
       "         'blog': 58,\n",
       "         'diffusers': 57,\n",
       "         'course': 38,\n",
       "         'datasets': 32,\n",
       "         'hub-docs': 18,\n",
       "         'hf-endpoints-documentation': 9,\n",
       "         'peft': 3,\n",
       "         'optimum': 2})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abddc1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mHow to invoke hf endpoints?\u001b[0m\n",
      "\u001b[33;1mCalling tool: 'retriever' with arguments: {'query': {'type': 'text', 'description': 'How to invoke Hugging Face endpoints?'}, 'source': 'hf-endpoints-documentation'}\u001b[0m\n",
      "\u001b[31;20mError in tool call execution: Your search query must be a string\n",
      "You should only use this tool with a correct input.\n",
      "As a reminder, this tool's description is the following:\n",
      "\n",
      "- retriever: Retrieves some documents from the knowledge base that have the closest embeddings to the input query.\n",
      "    Takes inputs: {'query': {'type': 'text', 'description': 'The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.'}, 'source': {'type': 'text', 'description': \"The source of the documents to search, as a str representation of a list. Possible values in the list are: ['peft', 'gradio', 'diffusers', 'datasets', 'deep-rl-class', 'blog', 'course', 'optimum', 'hub-docs', 'hf-endpoints-documentation', 'evaluate', 'transformers', 'datasets-server', 'pytorch-image-models']. If this argument is not provided, all sources will be searched.\"}}\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/agents.py\", line 502, in execute_tool_call\n",
      "    observation = self.toolbox.tools[tool_name](**arguments)\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/tools.py\", line 134, in __call__\n",
      "    outputs = self.forward(*args, **kwargs)\n",
      "  File \"/var/folders/bn/3fzk51790p77cl864k6kblr80000gn/T/ipykernel_85082/410724278.py\", line 28, in forward\n",
      "    assert isinstance(query, str), \"Your search query must be a string\"\n",
      "AssertionError: Your search query must be a string\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/agents.py\", line 951, in step\n",
      "    observation = self.execute_tool_call(tool_name, arguments)\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/agents.py\", line 505, in execute_tool_call\n",
      "    raise AgentExecutionError(\n",
      "transformers.agents.agents.AgentExecutionError: Error in tool call execution: Your search query must be a string\n",
      "You should only use this tool with a correct input.\n",
      "As a reminder, this tool's description is the following:\n",
      "\n",
      "- retriever: Retrieves some documents from the knowledge base that have the closest embeddings to the input query.\n",
      "    Takes inputs: {'query': {'type': 'text', 'description': 'The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.'}, 'source': {'type': 'text', 'description': \"The source of the documents to search, as a str representation of a list. Possible values in the list are: ['peft', 'gradio', 'diffusers', 'datasets', 'deep-rl-class', 'blog', 'course', 'optimum', 'hub-docs', 'hf-endpoints-documentation', 'evaluate', 'transformers', 'datasets-server', 'pytorch-image-models']. If this argument is not provided, all sources will be searched.\"}}\n",
      "\u001b[33;1mCalling tool: 'retriever' with arguments: {'query': {'type': 'text', 'description': 'Invoke Hugging Face endpoints'}, 'source': 'hf-endpoints-documentation'}\u001b[0m\n",
      "\u001b[31;20mError in tool call execution: Your search query must be a string\n",
      "You should only use this tool with a correct input.\n",
      "As a reminder, this tool's description is the following:\n",
      "\n",
      "- retriever: Retrieves some documents from the knowledge base that have the closest embeddings to the input query.\n",
      "    Takes inputs: {'query': {'type': 'text', 'description': 'The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.'}, 'source': {'type': 'text', 'description': \"The source of the documents to search, as a str representation of a list. Possible values in the list are: ['peft', 'gradio', 'diffusers', 'datasets', 'deep-rl-class', 'blog', 'course', 'optimum', 'hub-docs', 'hf-endpoints-documentation', 'evaluate', 'transformers', 'datasets-server', 'pytorch-image-models']. If this argument is not provided, all sources will be searched.\"}}\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/agents.py\", line 502, in execute_tool_call\n",
      "    observation = self.toolbox.tools[tool_name](**arguments)\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/tools.py\", line 134, in __call__\n",
      "    outputs = self.forward(*args, **kwargs)\n",
      "  File \"/var/folders/bn/3fzk51790p77cl864k6kblr80000gn/T/ipykernel_85082/410724278.py\", line 28, in forward\n",
      "    assert isinstance(query, str), \"Your search query must be a string\"\n",
      "AssertionError: Your search query must be a string\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/agents.py\", line 758, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/agents.py\", line 951, in step\n",
      "    observation = self.execute_tool_call(tool_name, arguments)\n",
      "  File \"/Users/sridharkannam/anaconda3/envs/py99/lib/python3.10/site-packages/transformers/agents/agents.py\", line 505, in execute_tool_call\n",
      "    raise AgentExecutionError(\n",
      "transformers.agents.agents.AgentExecutionError: Error in tool call execution: Your search query must be a string\n",
      "You should only use this tool with a correct input.\n",
      "As a reminder, this tool's description is the following:\n",
      "\n",
      "- retriever: Retrieves some documents from the knowledge base that have the closest embeddings to the input query.\n",
      "    Takes inputs: {'query': {'type': 'text', 'description': 'The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.'}, 'source': {'type': 'text', 'description': \"The source of the documents to search, as a str representation of a list. Possible values in the list are: ['peft', 'gradio', 'diffusers', 'datasets', 'deep-rl-class', 'blog', 'course', 'optimum', 'hub-docs', 'hf-endpoints-documentation', 'evaluate', 'transformers', 'datasets-server', 'pytorch-image-models']. If this argument is not provided, all sources will be searched.\"}}\n",
      "\u001b[33;1mCalling tool: 'retriever' with arguments: {'query': 'How to invoke Hugging Face endpoints', 'source': 'hf-endpoints-documentation'}\u001b[0m\n",
      "\u001b[33;1mCalling tool: 'final_answer' with arguments: To invoke Hugging Face endpoints, follow these steps: 1. Create an Endpoint by entering the Hugging Face Repository ID and your desired endpoint name. 2. Deploy a model (e.g., distilbert-base-uncased-finetuned-sst-2-english for text classification). 3. Test your Endpoint using the Inference widget in the overview.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:\n",
      "To invoke Hugging Face endpoints, follow these steps: 1. Create an Endpoint by entering the Hugging Face Repository ID and your desired endpoint name. 2. Deploy a model (e.g., distilbert-base-uncased-finetuned-sst-2-english for text classification). 3. Test your Endpoint using the Inference widget in the overview.\n"
     ]
    }
   ],
   "source": [
    "agent_output = agent.run(\"How to invoke hf endpoints?\")\n",
    "\n",
    "print(\"Final output:\")\n",
    "print(agent_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85eaf2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thought: I need to find information on how to invoke Hugging Face (hf) endpoints. I will use the `retriever` tool to search for relevant documents that provide guidance on this topic.\\nAction:\\n{\\n  \"action\": \"retriever\",\\n  \"action_input\": {\"query\": {\"type\": \"text\", \"description\": \"How to invoke Hugging Face endpoints?\"}, \"source\": \"hf-endpoints-documentation\"}\\n}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.logs[1]['llm_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86429616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thought: I have retrieved some documents that contain information about creating and invoking Hugging Face endpoints. I will now summarize the key steps for invoking these endpoints based on the retrieved information.\\nAction:\\n{\\n  \"action\": \"final_answer\",\\n  \"action_input\": \"To invoke Hugging Face endpoints, follow these steps: 1. Create an Endpoint by entering the Hugging Face Repository ID and your desired endpoint name. 2. Deploy a model (e.g., distilbert-base-uncased-finetuned-sst-2-english for text classification). 3. Test your Endpoint using the Inference widget in the overview.\"\\n}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.logs[4]['llm_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206867d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fa339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18672dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7208c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43d2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py99",
   "language": "python",
   "name": "py99"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
